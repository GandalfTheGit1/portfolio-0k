{
  "posts": [
    {
      "id": "standard-rag",
      "slug": "standard-rag",
      "title": "Standard RAG: The Foundation of Retrieval-Augmented Generation",
      "excerpt": "Understand Standard RAG — the foundation of modern retrieval-augmented generation. Learn how to build it in N8N, where it shines, and its limitations.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Standard RAG pairs an LLM with a retriever (often a vector DB) to ground answers in external documents, reducing hallucinations and enabling fresh, domain-specific knowledge."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Standard RAG?",
            "content": "A retrieve-then-generate pipeline: split text, embed chunks, store in a vector index, perform similarity (or hybrid) search for a query, then pass retrieved context + question to the LLM for an answer."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Standard RAG",
            "isList": true,
            "items": [
              "Open-domain QA and document search",
              "Customer support chatbots grounded in KBs",
              "Research assistants requiring up-to-date facts",
              "Legal/medical Q&A where citations are needed"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Standard RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Preprocess: split documents into chunks (Function/Built-in nodes)",
              "Embed chunks and store in a vector DB (e.g., Chroma, Pinecone)",
              "On query: compute embedding and run top-K similarity search",
              "Optionally fuse with BM25/hybrid ranking or rerank",
              "Concatenate top results into prompt and call LLM",
              "Return answer with citations; log for evaluation"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: accesses fresh domain data without training, reduces hallucinations, simple and widely applicable. Weaknesses: hinges on retrieval quality and index freshness; too many/irrelevant chunks can harm results."
          },
          {
            "type": "section",
            "id": "patterns",
            "title": "Implementation Patterns",
            "isList": true,
            "items": [
              "Hybrid retrieval (Embeddings + BM25) and Reciprocal Rank Fusion",
              "Reranking top candidates with an LLM or learned reranker",
              "Context compression to fit token limits",
              "Citation formatting and logging for audits"
            ]
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Retrieval precision/recall and hit rate",
              "Answer accuracy (e.g., F1 on QA sets)",
              "Factuality/hallucination rate",
              "End-to-end latency and token cost"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "AI",
        "N8N",
        "Vector Database",
        "BM25"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "12 min",
      "category": "IA"
    },
    {
      "id": "fusion-rag",
      "slug": "fusion-rag",
      "title": "Fusion RAG: Combining Multiple Data Sources for Smarter Retrieval",
      "excerpt": "Fusion RAG merges multiple retrieval sources—vector, keyword, and API—to produce more comprehensive, bias-resistant answers.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Fusion RAG enhances standard retrieval-augmented generation by pulling evidence from multiple, heterogeneous data sources—vector databases, keyword search, APIs, and SQL queries—and merging them intelligently before answer generation."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Fusion RAG?",
            "content": "Fusion RAG fuses retrieved data from different systems (semantic search, keyword search, SQL, APIs) into a single ranked list of context snippets. The model then synthesizes an answer that reflects diverse perspectives — effectively ensembling retrieval."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Fusion RAG",
            "isList": true,
            "items": [
              "Finance: Merge market data, news feeds, analyst reports.",
              "Enterprise knowledge bases: Combine internal docs with public APIs or support portals.",
              "Scientific summarization: Blend structured (PubMed DB) and unstructured (arXiv PDFs) sources."
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Example N8N Workflow",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Parallel Retrieval Nodes: Vector Search node → semantic match from embeddings DB, HTTP Request node → keyword or external API results, Database node → structured query to SQL/Notion.",
              "Merge results with a Set or Merge node.",
              "Rerank results by relevance using a ChatGPT node or reranker model.",
              "Feed top-ranked docs to the LLM generator with labeled citations."
            ]
          },
          {
            "type": "section",
            "id": "patterns",
            "title": "Implementation Patterns",
            "isList": true,
            "items": [
              "Hybrid retrieval: combine semantic and keyword search via Reciprocal Rank Fusion.",
              "Multi-indexing: maintain several databases optimized for different query types.",
              "Fusion-in-Decoder (FiD): treat each document as a separate token sequence and fuse during decoding."
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: Broader coverage across diverse data domains, reduces single-source bias and improves completeness, flexible architecture adaptable to different industries. Weaknesses: Complexity in data merging and scoring, possible noise or contradictions from multiple sources, higher latency due to multiple API calls."
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "AI",
        "N8N",
        "Fusion",
        "Multi-Source"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "corrective-rag",
      "slug": "corrective-rag",
      "title": "Corrective RAG: Making LLMs Trustworthy Through Feedback Loops",
      "excerpt": "Learn how Corrective RAG adds self-checking feedback loops to Retrieval-Augmented Generation workflows, reducing hallucinations and improving factual reliability.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Corrective RAG introduces a self-verification step in retrieval-augmented generation systems. It uses a second LLM pass to validate that the generated output is actually supported by the retrieved sources — dramatically reducing hallucinations in domains where precision matters."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Corrective RAG?",
            "content": "Corrective RAG (CRAG) is a variation of the standard Retrieval-Augmented Generation architecture that adds a feedback or correction loop to verify the model's own responses against the sources retrieved."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When and Why to Use Corrective RAG",
            "isList": true,
            "items": [
              "Legal research: validating case references and precedents.",
              "Financial and compliance tools: confirming that advice aligns with verified data.",
              "Healthcare/medical apps: ensuring generated content matches verified studies.",
              "Enterprise knowledge bases: filtering outdated or irrelevant docs automatically."
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Implementation in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Retrieve documents from your vector database.",
              "Generate an initial answer using ChatGPT node.",
              "Verify using a second ChatGPT node to check if answer is fully supported.",
              "Add an If node: if verification = unsupported, route to another retrieval with expanded query.",
              "Optional: Filter low-scoring documents via LLM evaluation before regenerating."
            ]
          },
          {
            "type": "section",
            "id": "patterns",
            "title": "Implementation Patterns",
            "isList": true,
            "items": [
              "Partition retrieved text into knowledge strips and score each for relevance.",
              "Use a smaller retriever evaluator (LLM or embedding model) to grade documents.",
              "Trigger secondary retrieval only when confidence falls below threshold.",
              "Optionally integrate web search APIs (Tavily or Bing Search) when no relevant context found."
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: Significantly lowers hallucination rate in high-stakes use cases, explicitly verifies factual grounding using retrieved data, compatible with any standard RAG pipeline. Weaknesses: Increased latency from extra API calls and loops, more complex control flow, verifier accuracy depends on prompt design and LLM version."
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Verification",
        "Feedback",
        "LLM",
        "Accuracy"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "Will",
      "read_time": "13 min",
      "category": "IA"
    },
    {
      "id": "adversarial-rag",
      "slug": "adversarial-rag",
      "title": "Adversarial RAG: Harden Your System Against Injection & Poisoning",
      "excerpt": "Defend RAG against prompt injection, document poisoning, and exfiltration with layered defenses: sanitize → verify → filter → rerank → verify output.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Apply multi‑layer defenses across input, retrieval, rerank, and generation. Target ASR <5% with modest latency overhead (~150–250ms)."
          },
          {
            "type": "section",
            "id": "threats",
            "title": "Threat Model",
            "isList": true,
            "items": [
              "Prompt injection (direct/indirect)",
              "Document poisoning (backdoors, hidden instructions)",
              "Query manipulation (exfiltration, jailbreaks)",
              "Retrieval manipulation (adversarial suffixes)"
            ]
          },
          {
            "type": "section",
            "id": "defense",
            "title": "Layered Defense",
            "isList": true,
            "items": [
              "Input: allowlist chars, length caps, rate limits",
              "Retrieval: verified corpus, similarity thresholds, diversity",
              "Rerank: adversarial detector + content safety",
              "Generation: prompt isolation, output filtering, citation enforcement"
            ]
          },
          {
            "type": "section",
            "id": "tests",
            "title": "Adversarial Test Suite",
            "isList": true,
            "items": [
              "Prompt injection set (direct/nested/multi‑turn)",
              "Poisoning set (hidden instructions, semantic backdoors)",
              "Exfiltration attempts (PII/system prompt)",
              "Evasion (encoding tricks, multi‑language)"
            ]
          },
          {
            "type": "section",
            "id": "architecture",
            "title": "Defensive Architecture",
            "content": "Sanitize → Allowlist → Verified retrieval → Content filter → Adversarial rerank → Generation constraints → Output verify → Monitor."
          },
          {
            "type": "section",
            "id": "snippets",
            "title": "Snippets (Pseudo)",
            "isList": true,
            "items": [
              "sanitize(query): block phrases, cap length (≤500), rate‑limit",
              "filter(doc): regex for SYSTEM OVERRIDE, scripts, credentials → drop"
            ]
          },
          {
            "type": "section",
            "id": "adv-training",
            "title": "Adversarial Training",
            "isList": true,
            "items": [
              "Dataset: paraphrases, char perturbations, semantic mutations",
              "Sources: OWASP LLM, BadRAG/TrojanRAG, internal red team",
              "Train bin classifier for detection + fine‑tune reranker"
            ]
          },
          {
            "type": "section",
            "id": "layer-config",
            "title": "Layered Retrieval Config",
            "isList": true,
            "items": [
              "L1 Input: allowlist charset; length 10–500; 100 q/h/user",
              "L2 Retrieval: verified corpus; sim > 0.6; enforce diversity",
              "L3 Rerank: adversarial detector + content safety",
              "L4 Generation: prompt isolation, output filter, citations"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks & Targets",
            "isList": true,
            "items": [
              "Targets: ASR <5%, FPR <2%, detect <100ms, auto‑recovery <1h",
              "Example: sanitization → verification → rerank → filter reduces ASR to ~1–3% with ~150–250ms overhead"
            ]
          },
          {
            "type": "section",
            "id": "checklist",
            "title": "Security Checklist",
            "isList": true,
            "items": [
              "Input sanitization & rate limits",
              "Provenance + signatures; anomaly detection",
              "Monthly adversarial reranker updates",
              "Comprehensive logging; quarterly red team"
            ]
          },
          {
            "type": "section",
            "id": "cost",
            "title": "Cost Model (Illustrative)",
            "isList": true,
            "items": [
              "+$0.0001/query detector (~100ms GPU)",
              "+$0.00005/doc verification (hash)",
              "Monitoring: ~$500/month; IR: ~$5000/year avg"
            ]
          },
          {
            "type": "section",
            "id": "env",
            "title": "Environment Variables",
            "content": "ADVERSARIAL_MODEL_PATH=/models/detector.pkl | BLOCK_THRESHOLD=0.85 | MAX_QUERY_LENGTH=500 | RATE_LIMIT_WINDOW=3600 | RATE_LIMIT_MAX=100 | ALERT_WEBHOOK=https://…"
          },
          {
            "type": "section",
            "id": "ops",
            "title": "Ops & Incident Response",
            "isList": true,
            "items": [
              "Monitoring: anomaly alerts and block thresholds",
              "Containment: quarantine docs, disable features if needed",
              "Rollback: retain previous index/models 7–30 days",
              "Post‑mortem: document and retrain on new vectors"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Security",
        "Prompt Injection",
        "Poisoning"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "agentic-rag",
      "slug": "agentic-rag",
      "title": "Agentic RAG: Embedding RAG Within Deliberative AI Agents",
      "excerpt": "Learn how to embed RAG within deliberative AI agents that autonomously plan their retrieval steps and refine answers through reasoning.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Agentic RAG embeds RAG within a deliberative AI agent that plans its own retrieval steps. Use Cases: Dynamic, multi-step tasks like research assistants, coding helpers that fetch libraries/docs, planning tools, or any workflow where the query evolves. The agent iteratively refines its query, self-corrects, and uses external tools before finalizing the answer."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Agentic RAG?",
            "content": "Agentic RAG refers to adding AI agents that control retrieval. The LLM acts as a planner that decides when and what to retrieve, rather than having a fixed retrieval pipeline. It maintains memory of past answers and decides when to call retrievers or tools."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Agentic RAG",
            "isList": true,
            "items": [
              "Dynamic, multi-step research tasks where the query evolves.",
              "Coding assistants that fetch libraries and documentation on demand.",
              "Planning tools and multi-turn customer support scenarios.",
              "Any workflow where the AI needs to reason about what information is needed next."
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Agentic RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Initialize the workflow with a prompt asking the LLM what information it needs.",
              "Use a Loop node to repeat the process.",
              "The LLM decides: Should I retrieve? Which tool should I use?",
              "Execute the appropriate API call or database query.",
              "Feed results back to the LLM with the question: What information do I need next?",
              "Use If/Switch nodes to route based on LLM reasoning.",
              "Continue until the LLM determines it has sufficient information to answer."
            ]
          },
          {
            "type": "section",
            "id": "patterns",
            "title": "Implementation Patterns",
            "isList": true,
            "items": [
              "ReAct (Reasoning + Acting): Agent thinks through steps, then acts.",
              "Chain-of-thought reasoning: Instruct system to retrieve via explicit reasoning.",
              "Tool-use agents: Maintain a toolkit and let the agent decide which to call.",
              "Memory-augmented agents: Track past queries and answers to avoid redundant retrievals."
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: Highly flexible and autonomous, can iteratively refine queries, handles complex workflows by reasoning, can correct itself and adjust strategy. Weaknesses: Hard to control and debug, performance depends on agent reasoning, may loop indefinitely or go off-track, slower and costlier due to multiple steps, safety and sandboxing concerns."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Success rate on goals — did the agent answer correctly?",
              "Number of steps to solution — is it efficient?",
              "Time and cost per query — what's the overhead?",
              "Loop count — does it get stuck looping?",
              "Tool usage patterns — which retrievers/APIs does it favor?"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Agents",
        "ReAct",
        "Autonomous",
        "Multi-Step"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "15 min",
      "category": "IA"
    },
    {
      "id": "atlas",
      "slug": "atlas",
      "title": "ATLAS: Large‑Scale Retrieval‑Augmented Pretraining",
      "excerpt": "Jointly pretrain retriever + LM at scale so the model leverages external memory for few‑shot learning and robust factual recall.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "ATLAS trains retrieval and language modeling together at massive scale, yielding strong few‑shot performance with external memory."
          },
          {
            "type": "section",
            "id": "concept",
            "title": "Core Concept",
            "content": "External knowledge is available during pretraining; the model learns to retrieve and use it, reducing reliance on parametric memory alone."
          },
          {
            "type": "section",
            "id": "when",
            "title": "When to Use",
            "isList": true,
            "items": [
              "Knowledge‑intensive QA",
              "Long‑tail facts",
              "Low‑shot generalization"
            ]
          },
          {
            "type": "section",
            "id": "practical",
            "title": "Practitioner Path",
            "isList": true,
            "items": [
              "Use REALM‑style approximations",
              "Fine‑tune with retrieval",
              "Frozen retriever + LM tuning"
            ]
          },
          {
            "type": "section",
            "id": "cost",
            "title": "Cost & Latency (Illustrative)",
            "isList": true,
            "items": [
              "Full pretraining is expensive",
              "Practical fine‑tune path is cheaper",
              "Inference adds retrieval latency (~300–500ms)"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "ATLAS",
        "Pretraining",
        "Retriever"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "15 min",
      "category": "IA"
    },
    {
      "id": "auto-rag",
      "slug": "auto-rag",
      "title": "Auto RAG: Self-Managing Ingestion, Indexing, and Optimization",
      "excerpt": "Automate ingestion, smart chunking, deduplication, reindexing, and tuning. Keep retrieval fresh and performant with minimal ops.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Auto-detect changes, ingest and smart-chunk, deduplicate, rebuild indexes on schedule or metric drops, and auto-tune k/chunk/thresholds."
          },
          {
            "type": "section",
            "id": "problem",
            "title": "Problem It Solves",
            "isList": true,
            "items": [
              "Manual index ops are slow/expensive",
              "Stale KB lowers answer quality",
              "Degrading performance without monitoring"
            ]
          },
          {
            "type": "section",
            "id": "architecture",
            "title": "Architecture",
            "content": "Sources → Change Detection → Auto-Ingestion → Smart Chunking → Deduplication → Index Mgmt → Auto-Tuning → Monitoring"
          },
          {
            "type": "section",
            "id": "ingestion",
            "title": "Ingestion & Dedup",
            "isList": true,
            "items": [
              "Parsers per type (.pdf, .docx, .html)",
              "Exact hash + MinHash + semantic duplicate removal",
              "Batch updates during off-peak"
            ]
          },
          {
            "type": "section",
            "id": "change-detection",
            "title": "Change Detection",
            "isList": true,
            "items": [
              "File-level: mtime, content hash, size thresholds",
              "Doc-level: semantic diff; reindex changed sections only"
            ]
          },
          {
            "type": "section",
            "id": "reindex",
            "title": "Scheduled Reindex",
            "isList": true,
            "items": [
              "Triggers: time/event/quality/manual",
              "Blue/green build + smoke tests + gradual traffic shift",
              "Auto-rollback on regressions"
            ]
          },
          {
            "type": "section",
            "id": "autotune",
            "title": "Auto-Tuning",
            "isList": true,
            "items": [
              "Optimize k, chunk size/overlap, rerank threshold",
              "Bayesian optimization over validation queries",
              "Composite score: relevance, mrr, latency, cost"
            ]
          },
          {
            "type": "section",
            "id": "monitoring",
            "title": "Monitoring & Alerts",
            "isList": true,
            "items": [
              "Ingestion rate, index size, P50/P95/P99 latency",
              "Hit rate, error rate, cost spikes",
              "Alert when hit rate<0.65 or P95>3s"
            ]
          },
          {
            "type": "section",
            "id": "dynamic-chunking",
            "title": "Dynamic Chunking",
            "isList": true,
            "items": [
              "By headings if present",
              "Short paragraphs → larger chunks",
              "Very long paragraphs → smaller chunks",
              "Default 512–1024 tokens"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks (Illustrative)",
            "isList": true,
            "items": [
              "Ingestion rate ↑ over weeks",
              "P95 latency ↓",
              "Hit@10 ↑"
            ]
          },
          {
            "type": "section",
            "id": "env",
            "title": "Environment Variables",
            "content": "S3_BUCKET=my-docs-bucket | PINECONE_INDEX=auto-rag-index | EMBEDDING_MODEL=text-embedding-3-small | AUTO_TUNE_ENABLED=true | TUNE_SCHEDULE=\"0 2 * * 0\" | DEDUP_THRESHOLD=0.95 | MAX_EMBEDDING_RATE=10000/hour"
          },
          {
            "type": "section",
            "id": "failures",
            "title": "Failure Modes (Mitigations)",
            "isList": true,
            "items": [
              "Runaway reindex → debounce 5 min min.",
              "Embedding quota exhaustion → rate limit + cost alerts",
              "Index corruption → WAL + snapshots",
              "Over-dedup → manual review of sample pairs"
            ]
          },
          {
            "type": "section",
            "id": "tuning-results",
            "title": "Auto‑tuning Results (Example)",
            "content": "Baseline: k=10, chunk=512, overlap=10%, thr=0.60 → Hit: 68.2%, Lat: 2.1s. Optimized (50 trials): k=15, chunk=768, overlap=15%, thr=0.68 → Hit: 74.8% (+6.6%), Lat: 1.9s (‑9.5%)."
          },
          {
            "type": "section",
            "id": "cost",
            "title": "Monthly Cost Model (Example)",
            "isList": true,
            "items": [
              "Ingestion: change detect $10; parsing $50; embeddings $200",
              "Indexing: storage $70; weekly reindex $20",
              "Monitoring: metrics $50; alerting $10",
              "Auto‑tuning compute: ~$120",
              "Total: ~$520/month; saves ~20 eng hours/mo"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Automation",
        "Indexing",
        "Tuning"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "contextual-rag",
      "slug": "contextual-rag",
      "title": "Contextual RAG: Context-Preserved Retrieval that Finds What Matters",
      "excerpt": "Contextual RAG augments chunks with surrounding context and often uses hybrid retrieval + reranking to boost retrieval accuracy.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Prepend context snippets to chunks before indexing and combine dense + sparse retrieval; rerank to cut failed retrievals significantly."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Contextual RAG?",
            "content": "A retrieval technique that preserves broader context within each chunk so similarity search has more clues, combined with BM25 and reranking."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Contextual RAG",
            "isList": true,
            "items": [
              "Long enterprise docs and manuals",
              "Legal or technical content where paragraphs lose meaning alone",
              "Customer support over large knowledge bases"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Contextual RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Generate short context blurbs per chunk (preprocessing step)",
              "Index contextualized chunks in vector DB and a BM25 index",
              "At query: perform hybrid search and apply Reciprocal Rank Fusion",
              "Rerank top candidates and generate with citations"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: large gains in retrieval success and end-to-end accuracy. Weaknesses: extra preprocessing cost, dual indices, relies on blurb quality."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Retrieval success rate and recall",
              "Answer F1 with/without contextualization",
              "Latency impact from hybrid + rerank"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Contextual Embeddings",
        "Hybrid",
        "Rerank"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "13 min",
      "category": "IA"
    },
    {
      "id": "feedback-based-rag",
      "slug": "feedback-based-rag",
      "title": "Feedback-Based RAG: Self-Improving Retrieval with User Signals",
      "excerpt": "Use explicit and implicit user feedback to rerank, retrain, and continuously improve RAG quality. Expect steady NDCG gains month over month.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Collect explicit (ratings) and implicit (clicks, dwell) signals, convert them to rewards, rerank in real time, and batch‑retrain weekly for durable gains."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What is Feedback-Based RAG?",
            "content": "A RAG system that learns from usage. User feedback updates retrieval scores/rerankers online and via periodic retraining."
          },
          {
            "type": "section",
            "id": "signals",
            "title": "Feedback Signals",
            "isList": true,
            "items": [
              "Explicit: thumbs, stars, helpful",
              "Implicit: clicks, dwell >30s, copy/share",
              "Negative: immediate back/ refine query",
              "Combined score: 0.6×explicit + 0.4×implicit"
            ]
          },
          {
            "type": "section",
            "id": "updates",
            "title": "Update Mechanisms",
            "isList": true,
            "items": [
              "Online: EMA update of retrieval scores (fast, noisy)",
              "Offline: daily aggregation + weekly reranker retrain",
              "Cadence: A/B test and roll back if metrics regress"
            ]
          },
          {
            "type": "section",
            "id": "data-requirements",
            "title": "Data Requirements",
            "isList": true,
            "items": [
              "Log: user_id, query, doc_ids, scores, rating, timestamp",
              "Thresholds: ≥100 feedback samples before retraining",
              "Temporal decay: weight recent feedback higher (exp. decay)"
            ]
          },
          {
            "type": "section",
            "id": "retrieval-config",
            "title": "Retrieval Configuration",
            "isList": true,
            "items": [
              "Base: BM25 + Dense (e.g., Contriever)",
              "Feedback layer: fine‑tuned reranker on feedback data",
              "Cold start: content similarity until ≥50 ratings",
              "Ensemble: 0.5×base + 0.3×feedback + 0.2×recency"
            ]
          },
          {
            "type": "section",
            "id": "env",
            "title": "Environment Variables",
            "content": "FEEDBACK_DB_URL=postgresql://… | MIN_FEEDBACK_COUNT=50 | DECAY_RATE=0.95 | UPDATE_SCHEDULE=\"0 2 * * *\""
          },
          {
            "type": "section",
            "id": "guardrails",
            "title": "Guardrails",
            "isList": true,
            "items": [
              "Outlier detection and velocity limits",
              "Honeypots to catch bots; rate limiting",
              "Temporal decay so recent feedback counts more"
            ]
          },
          {
            "type": "section",
            "id": "cost",
            "title": "Cost Model (100K monthly queries)",
            "isList": true,
            "items": [
              "Storage: ~100MB/month ≈ $0.01",
              "Daily batch: ≈ $3/month",
              "Weekly reranker retrain: ≈ $20/month",
              "Total: ≈ $23/month"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks (Illustrative)",
            "isList": true,
            "items": [
              "NDCG@10: +8–16% over 4–8 weeks",
              "Coverage: % results with ratings ↑",
              "Conversion metrics (positive action) ↑"
            ]
          },
          {
            "type": "section",
            "id": "ndcg-series",
            "title": "NDCG Weekly Deltas (Example)",
            "isList": true,
            "items": [
              "Week 0: 0.41",
              "Week 1: 0.45 (+0.04)",
              "Week 2: 0.48 (+0.03)",
              "Week 4: 0.51 (+0.03)"
            ]
          },
          {
            "type": "section",
            "id": "security",
            "title": "Security & Privacy",
            "isList": true,
            "items": [
              "Strip PII; store aggregates where possible",
              "GDPR: support delete requests",
              "Audit logs: retain original ratings separate from aggregates"
            ]
          },
          {
            "type": "section",
            "id": "ablation",
            "title": "Ablation (Illustrative)",
            "isList": true,
            "items": [
              "Explicit only: +8.2% (low noise)",
              "Implicit only: +5.1% (med noise)",
              "Combined: +12.4% (med noise)",
              "+ Temporal decay: +14.7% (low noise)"
            ]
          },
          {
            "type": "section",
            "id": "n8n",
            "title": "Minimal n8n Workflow",
            "content": "Webhook → DB insert (feedback) → Daily aggregate → Weekly retrain reranker → Update vector metadata → A/B test."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "Real‑World Fits",
            "isList": true,
            "items": [
              "E‑commerce search: rank by purchases/engagement",
              "Docs/support: surface articles that resolve issues",
              "Academic: rank by saves/citations"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Feedback",
        "Rerank",
        "Optimization"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "13 min",
      "category": "IA"
    },
    {
      "id": "hierarchical-rag",
      "slug": "hierarchical-rag",
      "title": "Hierarchical RAG: Top-Down Indexing for Large Corpora",
      "excerpt": "Hierarchical RAG builds multi-level indexes (topic → section → paragraph) to narrow retrieval efficiently and preserve context.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Search top-level nodes first (documents/topics), then drill into relevant child sections for precise retrieval with lower noise."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Hierarchical RAG?",
            "content": "A top-down retrieval process using multi-level indices with summaries/metadata on parents and detailed content on children."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Hierarchical RAG",
            "isList": true,
            "items": [
              "Extremely large or structured corpora (manuals, encyclopedias)",
              "Knowledge graphs and nested topics",
              "Multi-hop questions needing drill-down"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Hierarchical RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Organize indices: level 1 (docs/topics), level 2 (sections), level 3 (paragraphs)",
              "First query: retrieve top parent nodes",
              "Second query: retrieve from children of selected parents",
              "Merge results and generate with citations"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: efficient narrowing, context preservation, reduced noise. Weaknesses: complex to build/maintain, latency from multi-stage queries, risk of missing if parent retrieval fails."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Retrieval efficiency vs flat index",
              "First-attempt hit rate at parent level",
              "End-to-end answer accuracy"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Hierarchy",
        "Indexing",
        "Scaling"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "12 min",
      "category": "IA"
    },
    {
      "id": "hybrid-rag",
      "slug": "hybrid-rag",
      "title": "Hybrid RAG: Blend Offline Docs, Web, APIs, and Databases",
      "excerpt": "Combine offline corpora with live web search, APIs, and databases via routing + merging. Boost freshness and coverage with controlled cost.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Route queries to needed sources (offline, web, DB, APIs), merge and deduplicate, rerank, then synthesize with citations."
          },
          {
            "type": "section",
            "id": "problem",
            "title": "Problem It Solves",
            "isList": true,
            "items": [
              "Static KBs go stale",
              "Single source misses info",
              "Need real-time (prices, weather)"
            ]
          },
          {
            "type": "section",
            "id": "architecture",
            "title": "Architecture",
            "content": "Query Router → Sources (Offline/Web/DB/API) → Result Merger → Reranker → LLM"
          },
          {
            "type": "section",
            "id": "routing",
            "title": "Routing",
            "isList": true,
            "items": [
              "Rule-based: heuristics for time/structured/location",
              "ML-based: classifier outputs source probabilities",
              "Always include offline as fallback"
            ]
          },
          {
            "type": "section",
            "id": "merging",
            "title": "Merging & Dedup",
            "isList": true,
            "items": [
              "Weighted by reliability",
              "Semantic dedup (cos>0.92)",
              "Rerank across sources"
            ]
          },
          {
            "type": "section",
            "id": "partitioning",
            "title": "Index Partitioning",
            "isList": true,
            "items": [
              "By source type (offline/web-cache/db)",
              "By freshness (realtime/daily/weekly/archival)",
              "By compliance (public/internal/confidential/regulated)"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks (Illustrative)",
            "isList": true,
            "items": [
              "+18% accuracy vs offline-only",
              "+45% freshness score",
              "~+800ms latency (parallel fetch)",
              "3x cost—optimize via caching & budgets"
            ]
          },
          {
            "type": "section",
            "id": "cost",
            "title": "Cost Model (per 1000 queries)",
            "isList": true,
            "items": [
              "Offline corpus: ~$2",
              "Web search: ~$50 (30% of queries @ $0.17)",
              "DB queries: ~$1",
              "APIs: ~$5",
              "LLM generation: ~$8",
              "Total: ~$66 (33x offline)"
            ]
          },
          {
            "type": "section",
            "id": "opt-cost",
            "title": "Optimized Cost",
            "isList": true,
            "items": [
              "Cache web 24h (−60%)",
              "Tune routing to minimize web calls",
              "Use cheaper APIs where possible",
              "Optimized total: ~ $26 / 1000 queries"
            ]
          },
          {
            "type": "section",
            "id": "security",
            "title": "Security & Privacy",
            "isList": true,
            "items": [
              "Data isolation by compliance",
              "Permission checks by source",
              "No PII in web queries",
              "Audit source access"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Hybrid",
        "Web",
        "API",
        "DB"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "interactive-rag",
      "slug": "interactive-rag",
      "title": "Interactive RAG: Conversational Retrieval for Live Assistants",
      "excerpt": "Interactive RAG runs retrieval each turn in a conversation and adapts with user feedback to clarify and refine answers.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Interactive RAG integrates retrieval into multi-turn conversations, preserving context and letting users steer follow-ups for better answers."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Interactive RAG?",
            "content": "A conversational setting where each user message triggers retrieval over a KB and chat history, then generation with updated context."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Interactive RAG",
            "isList": true,
            "items": [
              "Customer support assistants over internal KBs",
              "Tutoring systems enabling clarifying questions",
              "Any app needing live user feedback loops"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Interactive RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Trigger on each user message",
              "Retrieve from memory (chat turns) and static corpus",
              "Generate response and detect if user wants more detail",
              "Branch using If/Switch nodes to fetch more and continue"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: adjusts on the fly, captures corrections, feels assistant-like. Weaknesses: context management is hard, long chats increase token cost, evaluation is trickier."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Dialogue relevance and satisfaction",
              "Turn-level accuracy and retention of context",
              "Latency per turn and overall cost"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Chatbots",
        "Conversation",
        "Memory"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "12 min",
      "category": "IA"
    },
    {
      "id": "multi-pass-rag",
      "slug": "multi-pass-rag",
      "title": "Multi-Pass RAG: Iterative Retrieval That Boosts Accuracy",
      "excerpt": "Run 2–3 retrieval→generation passes. First pass casts a wide net; later passes rerank and refine. Expect ~15–25% accuracy gain at modest extra cost.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Iterative retrieval–generation in 2–3 passes: coarse dense retrieval → rerank → fine hybrid retrieval → verify & stop on confidence."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What is Multi-Pass RAG?",
            "content": "Multiple retrieval–generation cycles where each pass narrows scope and improves answer quality. Analogy: skim → deep read → cross‑reference."
          },
          {
            "type": "section",
            "id": "when-to-use",
            "title": "When to Use",
            "isList": true,
            "items": [
              "Multi‑hop or complex questions needing synthesis",
              "Single‑pass results are redundant or off‑topic",
              "High‑stakes answers where correctness matters"
            ]
          },
          {
            "type": "section",
            "id": "how-it-works",
            "title": "How It Works",
            "isList": true,
            "items": [
              "Pass 1: Dense retrieval with large k, larger chunks for coverage",
              "Rerank: Cross‑encoder or heuristics to prune",
              "Pass 2: Hybrid BM25+dense, smaller k and smaller chunks",
              "Verify/Stop: Confidence threshold (e.g., 0.85) or max passes"
            ]
          },
          {
            "type": "section",
            "id": "practical-config",
            "title": "Practical Configuration",
            "isList": true,
            "items": [
              "Chunks: 800–1500 tokens (pass 1), 200–500 tokens (pass 2+), 15–20% overlap",
              "Retrievers: Dense k=50–100 → Hybrid k=10–20",
              "Reranker: Cross‑encoder (e.g., ms‑marco MiniLM) threshold ~0.7",
              "Stop rule: confidence > 0.85 or passes ≥ 3"
            ]
          },
          {
            "type": "section",
            "id": "cost-latency",
            "title": "Cost & Latency (Rule of Thumb)",
            "isList": true,
            "items": [
              "1‑pass: ~1.2s, ~2k tokens",
              "2‑pass: ~2.8s, ~3.8k tokens, +15–25% accuracy",
              "3‑pass: ~4.5s, ~5k+ tokens; diminishing returns"
            ]
          },
          {
            "type": "section",
            "id": "example-metrics",
            "title": "Example Metrics (Illustrative)",
            "isList": true,
            "items": [
              "Hit@10: 60% → 78% (2‑pass) → 88% (3‑pass)",
              "Hallucination rate: 12% → 6% → 4%",
              "Citation coverage: 55% → 72% → 80%"
            ]
          },
          {
            "type": "section",
            "id": "risks",
            "title": "Failure Modes & Guards",
            "isList": true,
            "items": [
              "Infinite loops → cap at 3–4 passes",
              "Error amplification → enforce citations & verification",
              "Cost creep → per‑session budgets",
              "Over‑narrowing → keep result diversity during rerank"
            ]
          },
          {
            "type": "section",
            "id": "n8n",
            "title": "Minimal n8n Workflow",
            "content": "HTTP In → Embed → Vector Query (k=50) → Code (rerank) → Vector Query (k=10) → Chat → Code (confidence) → IF (stop/loop)."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "Real‑World Fits",
            "isList": true,
            "items": [
              "Legal: precedents → clauses → citations",
              "Medical: symptoms → conditions → guidelines",
              "Support: topic → exact SOP step"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Iteration",
        "Rerank",
        "Hybrid"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "12 min",
      "category": "IA"
    },
    {
      "id": "multi-source-rag",
      "slug": "multi-source-rag",
      "title": "Multi-Source RAG: Integrate Diverse Knowledge Bases and Modalities",
      "excerpt": "Multi-Source RAG retrieves from several knowledge sources or modalities and fuses them to answer complex queries.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Query multiple sources in parallel (internal docs, web, databases, even images), then merge and reconcile evidence before generation."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Multi-Source RAG?",
            "content": "Retrieval from multiple knowledge bases and/or modalities with fusion strategies to improve coverage and robustness."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Multi-Source RAG",
            "isList": true,
            "items": [
              "Assistants that combine company docs with web results",
              "Cross-referencing tasks needing multiple repositories",
              "Image+text scenarios and heterogeneous data"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Building Multi-Source RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Set up parallel retrieval nodes for each source",
              "Normalize and label results by source",
              "Merge candidates and optionally rerank with an LLM",
              "Prompt LLM to synthesize and reconcile conflicts"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: improved coverage and bias mitigation. Weaknesses: higher complexity, potential inconsistencies, extra latency and cost."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Synthesis quality and coherence",
              "Cross-source coverage and trust scoring",
              "Hallucination count and latency"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Fusion",
        "Multimodal",
        "Benchmarks"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "13 min",
      "category": "IA"
    },
    {
      "id": "raptor",
      "slug": "raptor",
      "title": "RAPTOR: Tree-Organized Retrieval with Recursive Summaries",
      "excerpt": "Build hierarchical summaries (leaf→root) and retrieve across levels for long docs and theme‑based queries.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Summarize documents into a tree (leaf→root). Retrieve at multiple abstraction levels, then rerank and synthesize."
          },
          {
            "type": "section",
            "id": "problem",
            "title": "Problem It Solves",
            "isList": true,
            "items": [
              "Very long docs",
              "Need high‑level + detailed answers",
              "Preserve structure lost by flat chunking"
            ]
          },
          {
            "type": "section",
            "id": "tree",
            "title": "Tree Indexing",
            "isList": true,
            "items": [
              "Level 0: chunks (200–400 tokens, 10% overlap)",
              "Level 1+: summaries of groups (5–10)",
              "Root: overall document summary"
            ]
          },
          {
            "type": "section",
            "id": "retrieval",
            "title": "Retrieval",
            "isList": true,
            "items": [
              "Embed nodes per level",
              "Search all levels, keep top‑k",
              "Rerank across levels and cite"
            ]
          },
          {
            "type": "section",
            "id": "metadata",
            "title": "Metadata Tracking",
            "content": "Track parent/child links, page ranges, section names for provenance."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "Use Cases",
            "isList": true,
            "items": [
              "Legal contracts",
              "Medical papers",
              "Employee handbooks"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "RAPTOR",
        "Hierarchy",
        "Summarization"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "react-rag",
      "slug": "react-rag",
      "title": "ReAct RAG: Reasoning + Acting with Tools and Retrieval",
      "excerpt": "Interleave chain‑of‑thought with actions (retrieval/tool calls). Route, fetch, and reason step‑by‑step for complex queries.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Think‑act loops: the model reasons, decides which tool to call (retriever, web, DB), observes results, and continues until ready to answer."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "Definition",
            "content": "ReAct interleaves natural‑language reasoning traces with explicit actions. In RAG, actions typically include retrieval and verification tools."
          },
          {
            "type": "section",
            "id": "policy",
            "title": "Action Policy (Typical)",
            "isList": true,
            "items": [
              "If missing info → retrieve",
              "If ambiguous → ask clarification",
              "If conflicting sources → fetch more + verify",
              "Stop when confidence ≥ threshold"
            ]
          },
          {
            "type": "section",
            "id": "tools",
            "title": "Tools",
            "isList": true,
            "items": [
              "Retriever: vector + BM25",
              "Web: search API",
              "DB: SQL or Graph",
              "Verifier: cross‑encoder or heuristic"
            ]
          },
          {
            "type": "section",
            "id": "prompts",
            "title": "Prompting Pattern",
            "isList": true,
            "items": [
              "System: you can call tools and must cite sources",
              "Format: Thought → Action → Observation → …",
              "Terminate with Answer + Citations"
            ]
          },
          {
            "type": "section",
            "id": "risks",
            "title": "Risks & Guards",
            "isList": true,
            "items": [
              "Looping → cap steps (≤8)",
              "Tool abuse → budgets per session",
              "Injection → sanitize tool inputs",
              "Latency → parallelize where safe"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks (Illustrative)",
            "isList": true,
            "items": [
              "Complex multi‑hop QA: +10–18% vs single‑pass",
              "Latency +600–1200ms",
              "Cost +20–40% (tool calls)"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "ReAct",
        "Agents",
        "Tools"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "realm",
      "slug": "realm",
      "title": "REALM: Retrieval-Augmented Pretraining Explained (Practical Guide)",
      "excerpt": "REALM jointly trains retriever + LM so the model learns when/what to retrieve during pretraining. Practical approximations included.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "REALM pretrains with retrieval in the loop. For practitioners, use retrieval‑augmented fine‑tuning or frozen retrievers."
          },
          {
            "type": "section",
            "id": "when",
            "title": "When It Matters",
            "isList": true,
            "items": [
              "Open-domain QA",
              "Knowledge‑intensive tasks",
              "Long‑tail facts",
              "Verifiable answers"
            ]
          },
          {
            "type": "section",
            "id": "practical",
            "title": "Practical Alternatives",
            "isList": true,
            "items": [
              "Retrieval‑augmented fine‑tuning",
              "Frozen retriever + LM fine‑tuning",
              "Two‑stage: train retriever then LM with retrieval"
            ]
          },
          {
            "type": "section",
            "id": "retrieval-config",
            "title": "Retrieval Config (Typical)",
            "isList": true,
            "items": [
              "Encoder: BERT‑base, dim=768",
              "Index: ScaNN/FAISS over 13M passages",
              "Top‑k: 5–10",
              "Scoring: dot product"
            ]
          },
          {
            "type": "section",
            "id": "cost-latency",
            "title": "Cost & Latency (Illustrative)",
            "isList": true,
            "items": [
              "Full REALM training is costly (~$50k)",
              "Practical fine‑tuning path ~$600",
              "Inference: retrieval ~$0.50/1k queries, LM $3/1k"
            ]
          },
          {
            "type": "section",
            "id": "security",
            "title": "Security",
            "isList": true,
            "items": [
              "Provenance checks on corpus",
              "Encrypt queries",
              "Rate‑limit + audit"
            ]
          },
          {
            "type": "section",
            "id": "insights",
            "title": "Key Insights",
            "isList": true,
            "items": [
              "Joint training helps vs. separate training",
              "Top‑k=5–10 is a sweet spot",
              "Async index updates are fine (periodic)",
              "Retriever+LM reduces need for huge params"
            ]
          },
          {
            "type": "section",
            "id": "benchmarks",
            "title": "Benchmarks (NQ, Illustrative)",
            "isList": true,
            "items": [
              "T5‑11B (no retrieval): EM 34.5%",
              "DPR + BERT: EM 41.5% / Top‑5 72.3%",
              "REALM (110M): EM 40.4% / Top‑5 71.8%",
              "REALM (330M): EM 42.1% / Top‑5 73.9%"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "REALM",
        "Pretraining",
        "Retriever"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "15 min",
      "category": "IA"
    },
    {
      "id": "replug",
      "slug": "replug",
      "title": "RePLUG: Retrieval for Black‑Box LLMs (Format and Plug)",
      "excerpt": "Optimize how retrieved docs are formatted and prepended to prompts for closed LLMs. Improve utility without changing the model.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Treat the LLM as a black box. Focus on retrieval quality and prompt packaging (ordering, chunk size, citations)."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "Definition",
            "content": "RePLUG adapts retrieval for APIs like GPT‑4 by optimizing doc selection and how context is plugged into prompts."
          },
          {
            "type": "section",
            "id": "formatting",
            "title": "Formatting Tactics",
            "isList": true,
            "items": [
              "Order by utility (rerank)",
              "Trim to budgets (token caps)",
              "Use headings + citations",
              "De‑duplicate and diversify"
            ]
          },
          {
            "type": "section",
            "id": "retrieval",
            "title": "Retrieval Setup",
            "isList": true,
            "items": [
              "Hybrid BM25 + dense",
              "Cross‑encoder reranker",
              "Top‑k by budget (k=3–8)",
              "Domain‑specific filters"
            ]
          },
          {
            "type": "section",
            "id": "evaluation",
            "title": "Evaluation",
            "isList": true,
            "items": [
              "Hit@k, NDCG",
              "Answer faithfulness/hallucination",
              "A/B test packaging variants"
            ]
          },
          {
            "type": "section",
            "id": "limits",
            "title": "Limits & Guards",
            "isList": true,
            "items": [
              "Context overflow",
              "Citation enforcement",
              "PII scrubbing",
              "Cost ceilings"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "RePLUG",
        "Black‑box",
        "Prompting"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "13 min",
      "category": "IA"
    },
    {
      "id": "self-rag",
      "slug": "self-rag",
      "title": "Self-RAG: Retrieve, Generate, Critique for Higher Factuality",
      "excerpt": "Self-RAG trains or prompts an LM to decide when to retrieve and to self-critique its outputs, improving factuality and control.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Self-RAG lets the model decide when to retrieve and when to critique its own outputs, reducing hallucinations via explicit reflection and control tokens/prompting."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Self-RAG?",
            "content": "A single LM (or orchestrated prompts) that interleaves retrieval and self-critique using special tokens or structured steps, adapting to the query on the fly."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Self-RAG",
            "isList": true,
            "items": [
              "Factual report writing and long-form educational content",
              "Systems needing explicit citation checks",
              "Iterative summarization and refinement tasks"
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Simulating Self-RAG in N8N",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Generate an initial answer (OpenAI node)",
              "Run a critique step: ‘Is the answer fully supported by sources?’",
              "If low confidence, trigger additional retrieval and regenerate",
              "Optionally mark segments with <retrieve>/<critique> style tokens"
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: adaptive retrieval and self-correction; improved factuality and controllable behavior. Weaknesses: complex design; may require custom training; higher latency and orchestration cost."
          },
          {
            "type": "section",
            "id": "metrics",
            "title": "Metrics to Track",
            "isList": true,
            "items": [
              "Factuality scores and citation precision",
              "Answer accuracy vs baseline RAG",
              "Additional latency/cost from critique steps"
            ]
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Self-critique",
        "Reflection",
        "Verifier"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "William Marrero Masferrer",
      "read_time": "14 min",
      "category": "IA"
    },
    {
      "id": "speculative-rag",
      "slug": "speculative-rag",
      "title": "Speculative RAG: Speed and Accuracy with Specialist–Generalist LLMs",
      "excerpt": "Learn how Speculative RAG uses a small and a large LLM together to balance latency, cost, and accuracy in retrieval-augmented systems.",
      "content": {
        "sections": [
          {
            "type": "section",
            "id": "tldr",
            "title": "TL;DR",
            "content": "Speculative RAG pairs a smaller, faster specialist LLM with a larger, more capable generalist LLM. The small model drafts answers across subsets of retrieved documents, and the large model verifies or fuses them into one final, accurate response. The result: faster generation, lower cost per token, and improved reliability."
          },
          {
            "type": "section",
            "id": "definition",
            "title": "What Is Speculative RAG?",
            "content": "Speculative RAG is an advanced RAG framework where two models collaborate. A lightweight specialist generates draft answers from document subsets, while a larger generalist verifies and refines the output. The technique originates from Google's speculative decoding research."
          },
          {
            "type": "section",
            "id": "use-cases",
            "title": "When to Use Speculative RAG",
            "isList": true,
            "items": [
              "Scientific and technical QA — literature review, patent search, troubleshooting.",
              "Customer support automation — fast initial answers validated by a stronger LLM.",
              "Document summarization — smaller model drafts summaries, larger model ensures accuracy."
            ]
          },
          {
            "type": "section",
            "id": "workflow",
            "title": "Example N8N Workflow",
            "isOrdered": true,
            "isList": true,
            "items": [
              "Retrieve documents via vector search (Supabase, Pinecone, etc.).",
              "Split or cluster results using a Function node (KMeans or simple batching).",
              "Parallel branches — each batch sent to a smaller LLM node (GPT-3.5-turbo-mini) to generate draft.",
              "Merge drafts using a Merge node.",
              "Verification pass — send all drafts to larger LLM (GPT-4 or Claude 3) for final answer synthesis."
            ]
          },
          {
            "type": "section",
            "id": "patterns",
            "title": "Implementation Patterns",
            "isList": true,
            "items": [
              "Cluster retrieval outputs → run small LLMs independently per cluster.",
              "Use generalist LLM for cross-draft comparison and synthesis.",
              "Evaluate confidence using consistency or citation matching in the prompt."
            ]
          },
          {
            "type": "section",
            "id": "strengths",
            "title": "Strengths & Weaknesses",
            "content": "Strengths: Up to 50% lower latency through parallel execution, improved factual robustness via multiple perspectives, cheaper overall for large query loads. Weaknesses: Pipeline complexity and orchestration between two models, degraded output if small model drafts are poor, increased maintenance burden."
          }
        ]
      },
      "published": true,
      "published_at": "2025-11-01T00:00:00Z",
      "tags": [
        "RAG",
        "Performance",
        "LLM",
        "Optimization",
        "Speculative Decoding"
      ],
      "created_at": "2025-11-01T00:00:00Z",
      "updated_at": "2025-11-01T00:00:00Z",
      "author": "Will",
      "read_time": "12 min",
      "category": "IA"
    }
  ]
}